{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Part-0:-All-imports-necessary\" data-toc-modified-id=\"Part-0:-All-imports-necessary-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Part 0: All imports necessary</a></span></li><li><span><a href=\"#Part-I:-Probabilistic-and-Statistical-Models-for-Outlier-Detection\" data-toc-modified-id=\"Part-I:-Probabilistic-and-Statistical-Models-for-Outlier-Detection-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part I: Probabilistic and Statistical Models for Outlier Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-extreme-value-analysis\" data-toc-modified-id=\"Univariate-extreme-value-analysis-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Univariate extreme-value analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Markov-inequality\" data-toc-modified-id=\"Markov-inequality-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Markov inequality</a></span></li><li><span><a href=\"#Chebychev-inequality\" data-toc-modified-id=\"Chebychev-inequality-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Chebychev inequality</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-II:-Linear-models-for-Outlier-Detection\" data-toc-modified-id=\"Part-II:-Linear-models-for-Outlier-Detection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Part II: Linear models for Outlier Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Class-SVM\" data-toc-modified-id=\"One-Class-SVM-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One-Class SVM</a></span></li></ul></li><li><span><a href=\"#Part-III:-Proximity-Based-Outlier-Detection\" data-toc-modified-id=\"Part-III:-Proximity-Based-Outlier-Detection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Part III: Proximity-Based Outlier Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Local-Outlier-Factor-(LOF)\" data-toc-modified-id=\"The-Local-Outlier-Factor-(LOF)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>The Local Outlier Factor (LOF)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Part 0: All imports necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Part I: Probabilistic and Statistical Models for Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Univariate extreme-value analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Markov inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tail inequalities can be used in order to bound the probability that a value in the tail of a probability distribution should be considered anomalous.\n",
    "\n",
    "The strength of a tail inequality depends on the number of assumptions made about the underlying random variable.\n",
    "\n",
    "Fewer assumptions lead to weaker inequalities but such inequalities apply to larger classes of random variables.\n",
    "\n",
    "For example, the **Markov** and **Chebychev** inequalities are weak inequalities but they apply to very large classes of random variables.\n",
    "\n",
    "On the other hand, the **Chernoff** bound and **Hoeffding** inequality are both stronger inequalities but they apply to restricted classes of random variables.\n",
    "\n",
    "**Theorem**: Let X be a random variable that takes on only non-negative random values. Then, for any constant $\\alpha$ satisfying $E[X] < \\alpha$, the following\n",
    "is true: $$P(X > \\alpha) \\leq \\frac{E[X]}{\\alpha}$$\n",
    "\n",
    "So for example you have some sample from a random variable that takes on only non-negative random values (say Log-normally distributed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = 2\n",
    "sigma = 0.7\n",
    "sample = np.random.lognormal(mean=mean, sigma=sigma, size=10000)\n",
    "\n",
    "count, bins, ignored = plt.hist(sample, 50, normed=1, facecolor='g', alpha=0.75)\n",
    "\n",
    "x = np.linspace(min(bins), max(bins), 10000)\n",
    "pdf = (np.exp(-(np.log(x) - mean)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, pdf, linewidth=2, color='r')\n",
    "plt.title('Some feature we need to clean from outliers')\n",
    "plt.xlim((0, 140))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "values, base = np.histogram(sample, normed=True, bins=50)\n",
    "cumulative = np.cumsum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(base[:-1], len(sample)-cumulative)\n",
    "plt.title('Some feature we need to clean from outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To be able to apply Markov inequality you need to know the true mean value (E[X]).\n",
    "\n",
    "In our case E[X] is equal to 2.\n",
    "\n",
    "All we need to do is to decide what number will be the threshold ($\\alpha$) for outlier detection.\n",
    "\n",
    "Suppose $\\alpha$ is equal to 20.\n",
    "\n",
    "Then according to Markov inequality $$P(X > 20) \\leq \\frac{1}{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Thus if we agreed that $\\frac{1}{10}$ is appropriate borderline between inliers and outliers then we can use it to detect the last ones.\n",
    "\n",
    "Pros:\n",
    "- only one assumption sould be verified (non-negativity);\n",
    "- only one true value should be known (mean);\n",
    "\n",
    "Cons:\n",
    "- it gives a threshold only for upper-tail;\n",
    "- it doesn't give a [supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum) for upper-tail boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Chebychev inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The **Markov** inequality is defined only for probability distributions of non-negative values and provides a bound only on the upper tail.\n",
    "\n",
    "In practice, it is often desired to bound both tails of arbitrary distributions.\n",
    "\n",
    "Consider the case where X is an arbitrary random variable, which is not necessarily non-negative.\n",
    "\n",
    "In such cases the **Markov** inequality cannot be used directly.\n",
    "\n",
    "However, the (related) **Chebychev** inequality is very useful in such cases.\n",
    "\n",
    "The **Chebychev** inequality is a direct application of the **Markov** inequality to a non-negative derivative of random variable X\n",
    "\n",
    "**Theorem:** Let X be an arbitrary random variable. Then, for any constant $\\alpha$, the following is true:\n",
    "$$P(\\lvert X - E[X] \\rvert > \\lvert \\alpha \\rvert) \\leq \\frac{Var[X]}{\\alpha^2}$$\n",
    "\n",
    "So again in our case:\n",
    "- E[X] = 2;\n",
    "- Var[X] = 0.7;\n",
    "- $\\alpha$ = 20.\n",
    "\n",
    "Then according to **Chebychev** inequality $$P(\\lvert X - 2 \\rvert >  20) \\leq \\frac{0.7}{20^2} = \\frac{7}{4000}$$\n",
    "\n",
    "As you can see **Chebychev** inequality gives significantly smaller probability for the same $\\alpha$.\n",
    "\n",
    "Pros:\n",
    "- random variable doesn't have to be non-negative;\n",
    "- it gives a threshold for both upper-tail and lower-tail;\n",
    "\n",
    "Cons:\n",
    "- true values of E[X] and Var[X] should be known;\n",
    "- it doesn't give a\n",
    "[supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum)\n",
    "and\n",
    "[infimum](https://en.wikipedia.org/wiki/Infimum_and_supremum)\n",
    "for upper-tail and lower-tail boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Part II: Linear models for Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## One-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One-Class SVM is an example of Unsupervised Outlier Detection.\n",
    "\n",
    "The main issue this method is trying to solve can be described in following words:\n",
    "\n",
    "first look at our problem situation; we would like to determine whether (new) test data is member of a specific class, determined by our training data, or is not.\n",
    "\n",
    "Why would we want this? Imagine a factory type of setting; heavy machinery under constant surveillance of some advanced system.\n",
    "\n",
    "The task of the controlling system is to determine when something goes wrong; the products are below quality, the machine produces strange vibrations or something like a temperature that rises.\n",
    "\n",
    "It is relatively easy to gather training data of situations that are OK; it is just the normal production situation.\n",
    "\n",
    "But on the other side, collection example data of a faulty system state can be rather expensive, or just impossible.\n",
    "\n",
    "If a faulty system state could be simulated, there is no way to guarantee that all the faulty states are simulated and thus recognized in a traditional two-class problem.\n",
    "\n",
    "To cope with this problem, one-class classification problems (and solutions) are introduced.\n",
    "\n",
    "By just providing the normal training data, an algorithm creates a (representational) model of this data.\n",
    "\n",
    "If newly encountered data is too different, according to some measurement, from this model, it is labeled as out-of-class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So first thing we need to do is to create a coordinate grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then here we create a main dataset of \"normal\" (or usual) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We also need to generate a bunch on new (but still normal) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And here comes a bunch of outliers that was generated from another distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create an outlier detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we just generate our predictions for old usual data points, new usual data points and outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And calculate the number of errors that were made during predictions generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_error_train = len(y_pred_train[y_pred_train == -1])\n",
    "n_error_test = len(y_pred_test[y_pred_test == -1])\n",
    "n_error_outliers = len(y_pred_outliers[y_pred_outliers == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we return values of a decision function that we have got after fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The last part is to take a look at results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Outlier/Novelty Detection with One-Class SVM\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7))\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s, edgecolors='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s, edgecolors='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend(\n",
    "    [a.collections[0], b1, b2, c],\n",
    "    [\"learned frontier\", \"training observations\", \"new regular observations\", \"new abnormal observations\"],\n",
    "    loc=\"upper left\",\n",
    "    prop=matplotlib.font_manager.FontProperties(size=11)\n",
    ")\n",
    "plt.xlabel(\"error train: %d/100 ; errors novel regular: %d/20 ; errors novel abnormal: %d/20\" % (n_error_train, n_error_test, n_error_outliers))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see the algorithm was able to detect correctly normal new entities and outliers but made 19 mistakes on the old entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Part III: Proximity-Based Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Proximity-based techniques define a data point as an outlier, if its locality (or proximity) is sparsely populated.\n",
    "The proximity of a data point may be defined in a variety of ways, which are subtly different from one another, but are similar enough to be considered as one algorithm family.\n",
    "The most common ways of defining proximity for outlier analysis are as follows:\n",
    "\n",
    "* **Cluster-based:** The non-membership of a data point in any cluster, its distance from other clusters, and the size of the closest cluster, are used as criteria in order to compute the outlier score. The clustering problem has a complementary relationship to the outlier detection problem, in which points either belong to clusters or outliers.\n",
    "* **Distance-based:** The distance of a data point to its k-nearest neighbor (or other variant) is used in order to define proximity. Data points with large k-nearest neighbor distances are defined as outliers. Distance-based algorithms typically perform the analysis at a much more detailed granularity than the other two methods. On the other hand, this greater granularity often comes at a significant computational cost.\n",
    "* **Density-based:** The number of other points within a specified local region (grid region or distance-based region) of a data point,is used in order to define local density. These local density values may be converted into outlier scores. Other kernel-based methods or statistical methods for density estimation may also be used. The major difference between clustering and density-based methods is that clustering methods partition the data points, whereas densitybased methods partition the data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The LOF algorithm is an unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outlier samples that have a substantially lower density than their neighbors.\n",
    "\n",
    "The number of neighbors considered, (parameter n_neighbors) is typically chosen:\n",
    "\n",
    "    1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster;\n",
    "\n",
    "    2) smaller than the maximum number of close by objects that can potentially be local outliers.\n",
    "\n",
    "In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again initialize parameters for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "outliers_fraction = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a coordinate grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define which part of the sample belongs to inliers/outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_inliers = int((1. - outliers_fraction) * n_samples)\n",
    "n_outliers = int(outliers_fraction * n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define ground-truth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ground_truth = np.ones(n_samples, dtype=int)\n",
    "ground_truth[-n_outliers:] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create an outlier detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate inliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X1 = 0.3 * np.random.randn(n_inliers // 2, 2)\n",
    "X2 = 0.3 * np.random.randn(n_inliers // 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = np.r_[X1, X2]\n",
    "X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit_predict(X)\n",
    "scores_pred = clf.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threshold = stats.scoreatpercentile(scores_pred, 100 * outliers_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_errors = (y_pred != ground_truth).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Outlier/Novelty Detection with Local Outlier Factor (LOF)\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7))\n",
    "a = plt.contour(xx, yy, Z, levels=[threshold], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='orange')\n",
    "b = plt.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white', s=20, edgecolor='k')\n",
    "c = plt.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black', s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.legend(\n",
    "    [a.collections[0], b, c],\n",
    "    ['learned decision function', 'true inliers', 'true outliers'],\n",
    "    prop=matplotlib.font_manager.FontProperties(size=10),\n",
    "    loc='lower right'\n",
    ")\n",
    "plt.xlabel(\"%s (errors: %d)\" % (\"Local Outlier Factor (LOF)\", n_errors))\n",
    "plt.xlim((-7, 7))\n",
    "plt.ylim((-7, 7))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
